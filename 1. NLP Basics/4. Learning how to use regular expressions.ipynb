{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics: Learning how to use regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary reason that we're talking about regexes is in order to tokenize sentences or split a sentence into a list of words so that Python can understand what it needs to be looking at. Right now, Python just sees a string of characters, so we need to tell it what to focus on, and how to organize those characters. For our machine learning model, Python will need to split the string into what we call tokens, or words, so that the model can learn how those tokens relate to the r response variable. So we're going to use this re package, which provides a lot of the functions that we use along with regexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regular expressions in Python\n",
    "\n",
    "Python's `re` package is the most commonly used regex resource. More details can be found [here](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re_test = 'This is a made up string to test 2 different regex methods'\n",
    "re_test_messy = 'This      is a made up     string to test 2    different regex methods'\n",
    "re_test_messy1 = 'This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test) #'\\s' single whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test_messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above obtained result is not quite clean. Python doesn't know what to do with these extra white spaces that are included because this is only looking for a single white space. So a regex this simple won't work on this string. So we can just add a plus sign after the s in '\\s' and that will tell Python to look for one or more white spaces, so it should be able to handle this case where we have multiple white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_messy) #'\\s+' one or more whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On running the same regex on that last statement, it doesn't split it at all, it just returns the entire string all as one. When dealing with special characters as the separators between words, this simple regex can't handle that scenario. So, searching for white space isn't quite robust enough to handle the more complicated strings. \n",
    "\n",
    "So, instead of using '\\s', use '\\W', which will search for any non-word character, and it will use that to define its split, whether it's a space, a backslash, quotes, period, anything, it doesn't care. And we're going to keep this plus sign in there because we know that our string has more than one special characters in a row and if we remove that plus sign here, it won't be able to handle multiple special characters in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a recap, using the split() method allows us to tokenize by finding the characters that separate the words. The other option is, instead of searching for what splits the words, just search for the actual words themselves and ignore the characters that split the words. This can be done using the findall() method. The nice thing about this method is it's the same syntax as the split() method, so we'll just have to change the regex that we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capitalize S, instead of looking for one or more white space characters, \n",
    "#it looks for one or more non white space characters, i.e., words\n",
    "re.findall('\\S+', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous regex cannot handle this statement because it's looking for non white space characters, the dashes and backslashes and periods still count as non white space characters, so it thinks those are part of the word and returned the complete statement. \n",
    "\n",
    "So, we're going to switch from using Ss to Ws. Previously we learned that capital W plus looks for one or more non-word characters, so again, by making it lowercase instead of capital, it flips it, so this will search for one or more word characters, so basically it will search for tokens that resemble a word, so it will actually look for letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', re_test_messy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This explains how we can use two different methods from the re package along with several different regexes to properly tokenize messy sentences. \n",
    "* There are two methods from the re package that can be used for tokenizing. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# findall(): will search for the actual words while ignoring the things that separate the words. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# split(): will search for the characters that split the words while ignoring the actual words themselves. \n",
    "* And the regexes that are most useful for tokenizing, anything that is using a<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# W is based on words. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# S is based on white spaces. \n",
    "* It is much more common to be using the W regex because it allows the flexibility for words to be separated by spaces, or special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other examples of regex methods\n",
    "\n",
    "- re.search()\n",
    "- re.match()\n",
    "- re.fullmatch()\n",
    "- re.finditer()\n",
    "- re.escape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
